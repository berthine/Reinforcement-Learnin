{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "Copy of AMMI_EXPRL_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berthine/Reinforcement-Learnin/blob/master/Copy_of_AMMI_EXPRL_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UWM-gGi76pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5EbzJ1A78Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/exploration')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDPYhYvR7yKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from riverswim import RiverSwim\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5twn7Qv7yKf",
        "colab_type": "text"
      },
      "source": [
        "# Finite-Horizon MDPs\n",
        "We consider finite horizon problems with horizon $H$. For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n",
        "\n",
        "The value of a policy or the optimal value function can be computed using *backward induction*.\n",
        "\n",
        "\n",
        "Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n",
        "$$\n",
        "V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n",
        "$$\n",
        "where $V_{H+1}(s) = 0$, for any $s$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l8433mz7yKh",
        "colab_type": "code",
        "outputId": "3eb91a22-9bf6-4dfc-8359-74e3673deea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "env = RiverSwim(6)\n",
        "H = 10\n",
        "print(\"Reward matrix: \", env.R.shape)\n",
        "print(env.R)\n",
        "print()\n",
        "print(\"Transition matrix: \", env.P.shape)\n",
        "print(\"Transitions probabilities for state s_1:\")\n",
        "print(env.P[1])\n",
        "\n",
        "print(env.P[0,1,1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward matrix:  (6, 2)\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "\n",
            "Transition matrix:  (6, 2, 6)\n",
            "Transitions probabilities for state s_1:\n",
            "[[1.   0.   0.   0.   0.   0.  ]\n",
            " [0.05 0.6  0.35 0.   0.   0.  ]]\n",
            "0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXHdYle7yKm",
        "colab_type": "text"
      },
      "source": [
        "# Backward induction (aka Value Iteration)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_KZXn8A7yKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_induction(P, R, H):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A)-dim matrix\n",
        "            H: horizon\n",
        "\n",
        "        Returns:\n",
        "            The optimal V-function: array of shape(horizon,s)\n",
        "            The optimal policy\n",
        "\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    V = np.zeros((H + 1, S))\n",
        "    for h in reversed(range(H)):\n",
        "        for s in range(S):\n",
        "            \"\"\" \n",
        "            Here, we compute V^*(h, s) using the Bellman optimality equation:\n",
        "\n",
        "            V[h, s] = max_a  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n",
        "            \"\"\"\n",
        "            for a in range(A):\n",
        "                tmp =  np.dot(P[s, a], R[s, a] + V[h + 1])\n",
        "                if (a == 0) or (tmp > V[h, s]):\n",
        "                    policy[h, s] = a\n",
        "                    V[h, s] = tmp\n",
        "    return V, policy\n",
        "\n",
        "def policy_evaluation(P, R, H, policy):\n",
        "    \"\"\"\n",
        "        policy =action taken\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A)-dim matrix\n",
        "            H: horizon\n",
        "            policy: policy (H,S)-dim matrix\n",
        "\n",
        "        Returns:\n",
        "            The V-function of the given policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    V = np.zeros((H + 1, S))\n",
        "    for h in reversed(range(H)):\n",
        "        for s in range(S):\n",
        "            \"\"\" \n",
        "            Here, we compute V^pi(h, s) using the Bellman equation for the policy pi:\n",
        "\n",
        "            a = policy[h,s]\n",
        "            V[h, s] =  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n",
        "            \"\"\"\n",
        "            a = policy[h,s]\n",
        "            # complete the policy evalution here\n",
        "            V[h,s ] = R[s,a] + (P[s,a, :]).dot(V[h+1, :])\n",
        "    return V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvmhRCN07yKs",
        "colab_type": "text"
      },
      "source": [
        "Compute solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVjkydge7yKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "885c6373-8859-42d6-b3d5-9fd5ca901bdf"
      },
      "source": [
        "Vstar, POLstar = backward_induction(env.P, env.R, H)\n",
        "\n",
        "print(\"Optimal policy\")\n",
        "print(np.round(Vstar))\n",
        "\n",
        "print(POLstar)\n",
        "\n",
        "# To test your implementation:\n",
        "V_policy = policy_evaluation(env.P, env.R, H, POLstar)\n",
        "# V_policy must be equal to Vstar\n",
        "print(\"difference V_policy - Vstar\")\n",
        "print(np.abs(V_policy - Vstar).sum())\n",
        "print(\".....\")\n",
        "print(np.round(V_policy))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal policy\n",
            "[[0. 1. 1. 2. 4. 5.]\n",
            " [0. 0. 1. 2. 3. 5.]\n",
            " [0. 0. 1. 2. 3. 4.]\n",
            " [0. 0. 1. 1. 3. 4.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 0. 1. 3.]\n",
            " [0. 0. 0. 0. 1. 2.]\n",
            " [0. 0. 0. 0. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "[[1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [0 1 1 1 1 1]\n",
            " [0 1 1 1 1 1]\n",
            " [0 0 1 1 1 1]\n",
            " [0 0 0 1 1 1]\n",
            " [0 0 0 0 1 1]\n",
            " [0 0 0 0 0 1]]\n",
            "difference V_policy - Vstar\n",
            "7.16093850883226e-15\n",
            ".....\n",
            "[[0. 1. 1. 2. 4. 5.]\n",
            " [0. 0. 1. 2. 3. 5.]\n",
            " [0. 0. 1. 2. 3. 4.]\n",
            " [0. 0. 1. 1. 3. 4.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 0. 1. 3.]\n",
            " [0. 0. 0. 0. 1. 2.]\n",
            " [0. 0. 0. 0. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG0Zc6uv79WV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "outputId": "873878d2-8c96-4e8d-a4c4-5ba3dbef33f7"
      },
      "source": [
        "S, A = env.R.shape #number of states and actions\n",
        "Phat = np.zeros((S,A, S))\n",
        "Rhat = np.zeros((S,A))\n",
        "\n",
        "N_sa = np.zeros((S,A))\n",
        "N_sas = np.zeros((S,A, S))\n",
        "S_sa = np.zeros((S,A))\n",
        "\n",
        "nb_episodes = 10\n",
        "##loop over episodes\n",
        "for ep in range(nb_episodes):\n",
        "  state = env.reset()\n",
        "  for h in range(H):\n",
        "    action = np.random.choice(A)\n",
        "    next_state , reward, done, _ = env.step(action)\n",
        "\n",
        "    #update estimates\n",
        "    N_sa[state, action] +=1\n",
        "    N_sas[state, action, next_state] +=1\n",
        "    S_sa [state, action] +=reward\n",
        "\n",
        "    Rhat[state, action] = S_sa[state, action]/N_sa[state, action]\n",
        "    Phat[state, action, :] = N_sas[state, action, :]/N_sa[state, action]\n",
        "    \n",
        "    state = next_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(Rhat)\n",
        "# print(Phat)\n",
        "# print(env.R)\n",
        "# check confidence interval for rewards with high probanility, we have\n",
        "#R(s,a) is in the interval [R_hat[s,a] -beta_r[s,a],  R_hat[s,a] + beta_r[s,a]]\n",
        "beta_r = np.zeros((S,A))\n",
        "beta_p = np.zeros((S,A))\n",
        "for s in range(S):\n",
        "  for a in range(A):\n",
        "    n = max(N_sa[s,a],1)\n",
        "    beta_r[s,a] = np.sqrt(np.log(S*A*n)/n)\n",
        "    beta_p[s,a] = np.sqrt(S*np.log(S*A*n)/n)\n",
        "\n",
        "print(\"\")\n",
        "print(Rhat)\n",
        "print(Rhat + beta_r)\n",
        "print(Rhat - beta_r)\n",
        "print(\"True R\")\n",
        "print(env.R)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  ||P(. | s,a) - Phat(.| s,a)||_1 <= beta_p(s,a)\n",
        "\"\"\"\n",
        "print(\"chechink probabilities\")\n",
        "for s in range(S):\n",
        "    for a in range(A):\n",
        "        norm_1 = np.abs(env.P[s, a, :] - Phat[s,a, :]).sum()\n",
        "        print(norm_1, beta_p[s,a])\n",
        "        assert( norm_1 <= beta_p[s,a])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]]\n",
            "[[0.46080036 0.38491113]\n",
            " [0.58838518 0.66625152]\n",
            " [0.98376839 1.57635867]\n",
            " [1.57635867 1.57635867]\n",
            " [1.57635867 1.57635867]\n",
            " [1.57635867 1.57635867]]\n",
            "[[-0.45080036 -0.38491113]\n",
            " [-0.58838518 -0.66625152]\n",
            " [-0.98376839 -1.57635867]\n",
            " [-1.57635867 -1.57635867]\n",
            " [-1.57635867 -1.57635867]\n",
            " [-1.57635867 -1.57635867]]\n",
            "True R\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "chechink probabilities\n",
            "0.0 1.116478311474126\n",
            "0.15238095238095228 0.9428358642241705\n",
            "0.0 1.4412434701867982\n",
            "0.10909090909090914 1.6319762569439942\n",
            "0.0 2.4097305899958688\n",
            "1.0 3.8612743879097744\n",
            "1.0 3.8612743879097744\n",
            "1.0 3.8612743879097744\n",
            "1.0 3.8612743879097744\n",
            "1.0 3.8612743879097744\n",
            "1.0 3.8612743879097744\n",
            "1.0 3.8612743879097744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1nCGWuw7yKy",
        "colab_type": "text"
      },
      "source": [
        "## UCRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoWtsh7v7yK0",
        "colab_type": "text"
      },
      "source": [
        "UCRL is an algorithm for efficient exploration in finite-horizon tabular MDP.\n",
        "In this setting, the regret is defined as\n",
        "$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n",
        "UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n",
        "\n",
        "The structure of the algorithm is as follow\n",
        "\n",
        "For $k = 1, \\ldots, K$ do<br>\n",
        "> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n",
        "> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",
        ">> for $h=1, \\ldots, H$<br>\n",
        ">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",
        ">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",
        ">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n",
        "\n",
        "<font color='#ed7d31'>Optimistic planning</font>\n",
        "At each episode, UCRL computes the optimal solution by solving the following \"extended\" problem\n",
        "$$\n",
        "V_h^\\star(s) =  \\max_{r \\in B_r(s,a)} r + \\max_{p \\in B_p(s,a)} \\sum_{s'} p(s') V_{h+1}(s') \n",
        "$$\n",
        "where $V_{H+1}(s) = 0$ and $B_r(s,a)$ and $B_p(s,a)$ are confidence intervals on the estimated transitions and rewards:\n",
        "\n",
        "$$\n",
        "B_r(s,a) = \\{ r(s,a):  |r_s, a) - \\hat{r}(s,a)| \\leq \\beta_r(s,a)  \\}\n",
        "$$\n",
        "\n",
        "$$\n",
        "B_p(s,a) = \\{ p(\\cdot|s,a):  ||p(\\cdot|s,a) - \\hat{p}(\\cdot|s,a)||_1 \\leq \\beta_p(s,a)  \\}\n",
        "$$\n",
        "where \n",
        "\n",
        "$$\n",
        "\\beta_r(s, a) = \\sqrt{ \\frac{ \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  } \\\\\n",
        "\\beta_p(s, a) = \\sqrt{ \\frac{  S \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  }\n",
        "$$\n",
        "and where  $N^+(s, a) = \\max(1, N(s, a))$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji7IShLN7yK1",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "The following function computes:\n",
        "$$\\max_{x \\in B_p} \\sum_{s'} x(s') V(s') $$\n",
        "where $B_p = [P-\\beta, P+\\beta]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57zVp32L7yK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LPprobaH(v, P, beta, verbose=0):\n",
        "    \"\"\"\n",
        "        max_x v^T x\n",
        "        s.t.    0 <= x_i <= 1\n",
        "                \\sum_i |x_i - p_i| \\leq beta\n",
        "                \\sum_i x_i = 1\n",
        "    \"\"\"\n",
        "    sorted_idxs = np.argsort(v)[::-1]\n",
        "\n",
        "    pest = P.copy()\n",
        "    idx = sorted_idxs[0]\n",
        "    pest[idx] = min(1., P[idx] + beta / 2.)\n",
        "    delta = pest.sum()\n",
        "    j = len(P)-1\n",
        "    while delta > 1:\n",
        "        idx_j = sorted_idxs[j]\n",
        "        m = max(0, 1. - delta + pest[idx_j])\n",
        "        delta = delta - pest[idx_j] + m\n",
        "        pest[idx_j] = m\n",
        "        j -= 1\n",
        "    w = np.dot(pest, v)\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk0v7jg67yK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def UCRL(mdp, H, nb_episodes, VSTAR=0):\n",
        "    S, A = mdp.Ns, mdp.Na\n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sas = np.zeros((S,A,S), dtype=np.int)\n",
        "    N_sa = np.zeros((S,A), dtype=np.int)\n",
        "    regret = np.zeros((nb_episodes,))\n",
        "    V = np.zeros((H + 1, S))\n",
        "    \n",
        "    S_sa = np.zeros((S,A))\n",
        "    \n",
        "    delta = 0.5\n",
        "    \n",
        "    for k in range(nb_episodes):\n",
        "        \n",
        "        # compute optimistic solution\n",
        "        # 1. compute confidence intervals\n",
        "        N = np.maximum(N_sa, 1)\n",
        "        LOGT = np.log(S * A * N / delta)\n",
        "        beta_r = np.sqrt( LOGT/N )\n",
        "        beta_p = np.sqrt( S*LOGT/N )\n",
        "        \n",
        "        # 2. run extended value iteration\n",
        "        V.fill(0)\n",
        "        for h in reversed(range(H)):\n",
        "            for s in range(S):\n",
        "                temp = np.zeros(A)\n",
        "                for a in range(A):\n",
        "                    dotp = LPprobaH(V[h + 1], Phat[s, a], beta_p[s, a])\n",
        "                    temp[a] = Rhat[s, a] + beta_r[s,a] + dotp\n",
        "                V[h, s] = temp.max() \n",
        "                policy[h, s] = temp.argmax()\n",
        "        # execute policy\n",
        "        initial_state = state = mdp.reset()\n",
        "        for h in range(H):\n",
        "            action = policy[h][state]\n",
        "            next_state, reward, done, _ = mdp.step(action)\n",
        "            \n",
        "            # update estimates (Phat, Rhat, N_sa, N_sas)\n",
        "            N_sa[state, action] += 1\n",
        "            N_sas[state, action, next_state] +=1\n",
        "            S_sa[state, action] += reward\n",
        "            \n",
        "            Rhat[state, action] = S_sa[state, action]/N_sa[state, action]\n",
        "            Phat[state, action, :] = N_sas[state, action, :]/N_sa[state, action]\n",
        "\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # update regret\n",
        "        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n",
        "        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n",
        "        \n",
        "        if k % 50 == 0:\n",
        "            print(\"regret[{}]: {}\".format(k, regret[k]))\n",
        "    return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-UTr3Gq7yK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad643f42-95f3-41a1-b8b7-35fd72bda001"
      },
      "source": [
        "nb_repetitions = 5\n",
        "nb_episodes = 750\n",
        "regrets = np.zeros((nb_repetitions, nb_episodes))\n",
        "for it in range(nb_repetitions):\n",
        "    print(\"Running simulation: {}\".format(it))\n",
        "    regrets[it] = UCRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running simulation: 0\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3444181033124999\n",
            "regret[100]: 0.031235826999999994\n",
            "regret[150]: 0.033321987750000004\n",
            "regret[200]: 0.031235826999999994\n",
            "regret[250]: 0.08074587581249998\n",
            "regret[300]: 0.031235826999999994\n",
            "regret[350]: 0.03272564771874997\n",
            "regret[400]: 0.09297454075\n",
            "regret[450]: 0.03272564771874997\n",
            "regret[500]: 0.031235826999999994\n",
            "regret[550]: 0.031235826999999994\n",
            "regret[600]: 0.03363150300000001\n",
            "regret[650]: 0.03272564771874997\n",
            "regret[700]: 0.031235826999999994\n",
            "Running simulation: 1\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.34541370037499985\n",
            "regret[100]: 0.0445777933125\n",
            "regret[150]: 0.08074587581249998\n",
            "regret[200]: 0.07835019981249997\n",
            "regret[250]: 0.03363150300000001\n",
            "regret[300]: 0.03363150300000001\n",
            "regret[350]: 0.03363150300000001\n",
            "regret[400]: 0.03306478075000002\n",
            "regret[450]: 0.03363150300000001\n",
            "regret[500]: 0.03363150300000001\n",
            "regret[550]: 0.03272564771874997\n",
            "regret[600]: 0.035121323718749986\n",
            "regret[650]: 0.09297454075\n",
            "regret[700]: 0.03363150300000001\n",
            "Running simulation: 2\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.14169135918749998\n",
            "regret[100]: 0.03363150300000001\n",
            "regret[150]: 0.031235826999999994\n",
            "regret[200]: 0.031235826999999994\n",
            "regret[250]: 0.03363150300000001\n",
            "regret[300]: 0.03363150300000001\n",
            "regret[350]: 0.03363150300000001\n",
            "regret[400]: 0.031235826999999994\n",
            "regret[450]: 0.031235826999999994\n",
            "regret[500]: 0.031235826999999994\n",
            "regret[550]: 0.035121323718749986\n",
            "regret[600]: 0.031235826999999994\n",
            "regret[650]: 0.035121323718749986\n",
            "regret[700]: 0.031235826999999994\n",
            "Running simulation: 3\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.34572321562499986\n",
            "regret[100]: 0.07835019981249997\n",
            "regret[150]: 0.08074587581249998\n",
            "regret[200]: 0.13206049799999997\n",
            "regret[250]: 0.10227136281249993\n",
            "regret[300]: 0.03363150300000001\n",
            "regret[350]: 0.031235826999999994\n",
            "regret[400]: 0.03363150300000001\n",
            "regret[450]: 0.1765559050312499\n",
            "regret[500]: 0.035121323718749986\n",
            "regret[550]: 0.031235826999999994\n",
            "regret[600]: 0.03363150300000001\n",
            "regret[650]: 0.03363150300000001\n",
            "regret[700]: 0.031235826999999994\n",
            "Running simulation: 4\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.14169135918749998\n",
            "regret[100]: 0.035194735281250056\n",
            "regret[150]: 0.03306478075000002\n",
            "regret[200]: 0.10466703881249992\n",
            "regret[250]: 0.04283525331249999\n",
            "regret[300]: 0.03363150300000001\n",
            "regret[350]: 0.031235826999999994\n",
            "regret[400]: 0.031235826999999994\n",
            "regret[450]: 0.03363150300000001\n",
            "regret[500]: 0.03363150300000001\n",
            "regret[550]: 0.03363150300000001\n",
            "regret[600]: 0.03272564771874997\n",
            "regret[650]: 0.035121323718749986\n",
            "regret[700]: 0.03363150300000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M49yfZ_S7yLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "48e96f9c-9c10-4edc-f5cc-62e5d487d8fa"
      },
      "source": [
        "x = regrets.cumsum(axis=-1)\n",
        "mean_regret = x.mean(axis=0)\n",
        "std_regret = x.std(axis=0)\n",
        "plt.plot(mean_regret)\n",
        "plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n",
        "plt.ylabel('regret')\n",
        "\n",
        "# SAVE PSRL REGRET\n",
        "ucrl_regret = mean_regret"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myPuG9Er7yLL",
        "colab_type": "text"
      },
      "source": [
        "# Posterior Sampling for RL\n",
        "\n",
        "At each iteration, PSRL samples one MDP from the posterior distribution and run the associated optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gxP6B9r7yLN",
        "colab_type": "text"
      },
      "source": [
        "Implement posterior sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl9YwtfJ7yLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PSRL(mdp, H, nb_episodes, VSTAR=0):\n",
        "    reward_prior = [1,1]\n",
        "    S, A = mdp.Ns, mdp.Na\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sas = np.zeros((S,A,S), dtype=np.int)\n",
        "    N_sa = np.zeros((S,A), dtype=np.int)\n",
        "    regret = np.zeros((nb_episodes,))\n",
        "    \n",
        "    for k in range(nb_episodes):\n",
        "        \n",
        "        # compute policy\n",
        "        # 1. sample MDP\n",
        "        R = np.zeros_like(Rhat)\n",
        "        P = np.zeros((S, A, S))\n",
        "        for s in range(S):\n",
        "            for a in range(A):\n",
        "                # sample transition matrix\n",
        "                # P[s, a] follows a dirichlet Dirichlet distribution of parameters N_sas[s, a,:] + 1\n",
        "                P[s, a] = ...\n",
        "\n",
        "                # posterior for Bernoulli rewards\n",
        "                N = N_sa[s, a]\n",
        "                v = N * Rhat[s, a]\n",
        "                a0 = reward_prior[0] + v\n",
        "                b0 = reward_prior[1] + N - v\n",
        "                p = np.random.beta(a=a0, b=b0, size=1).item()\n",
        "                R[s, a] = p\n",
        "        \n",
        "        # 2. compute optimal policy\n",
        "        V, policy = ...\n",
        "        \n",
        "        # execute policy\n",
        "        initial_state = state = mdp.reset()\n",
        "        for h in range(H):\n",
        "            action = policy[h][state]\n",
        "            next_state, reward, done, _ = mdp.step(action)\n",
        "            \n",
        "            # update estimates (Rhat, N_sa, N_sas)\n",
        "            ...\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # update regret\n",
        "        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n",
        "        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n",
        "        \n",
        "        if k % 50 == 0:\n",
        "            print(\"regret[{}]: {}\".format(k, regret[k]))\n",
        "\n",
        "    return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT38aVOB7yLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_repetitions = 5\n",
        "nb_episodes = 750\n",
        "regrets = np.zeros((nb_repetitions, nb_episodes))\n",
        "for it in range(nb_repetitions):\n",
        "    print(\"Running simulation: {}\".format(it))\n",
        "    regrets[it] = PSRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQhP7cpn7yLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = regrets.cumsum(axis=-1)\n",
        "mean_regret = x.mean(axis=0)\n",
        "std_regret = x.std(axis=0)\n",
        "plt.plot(mean_regret)\n",
        "plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n",
        "plt.ylabel('regret')\n",
        "\n",
        "# SAVE PSRL REGRET\n",
        "psrl_regret = mean_regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phHZWwdJ7yLc",
        "colab_type": "text"
      },
      "source": [
        "Compare algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvcqwaJY7yLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(ucrl_regret, label='UCRL-H')\n",
        "plt.plot(psrl_regret, label='PSRL')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it2tQtxL7yLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}