{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Copy of AIMS_2020 Exercice.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "puk7xD1EBDEu"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berthine/Reinforcement-Learnin/blob/master/RL_AIMS_2020_Exercice1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYW2YAMZOX4-",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning in Finite MDPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFu6XLHrtIa6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K4vLMPTSxmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1\n",
        "!cd mvarl_hands_on && git pull"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNsuKZxeOX4_",
        "colab_type": "text"
      },
      "source": [
        "## MDPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNlEnGsYOX5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/utils')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "from gridworld import GridWorldWithPits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puk7xD1EBDEu",
        "colab_type": "text"
      },
      "source": [
        "## Define the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezz31WR2uJFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from finite_env import FiniteEnv\n",
        "import numpy as np\n",
        "\n",
        "# Note: You do not need to read this Class. It only redefines the clearning robots\n",
        "\n",
        "\n",
        "class RobotEnv(FiniteEnv):\n",
        "    \"\"\"\n",
        "    Enviroment with 2 states and 3 actions\n",
        "    Args:\n",
        "        gamma (float): discount factor\n",
        "        seed    (int): Random number generator seed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=0.5, seed=42):\n",
        "        # Set seed\n",
        "        self.RS = np.random.RandomState(seed)\n",
        "\n",
        "        # Transition probabilities\n",
        "        # shape (Ns, Na, Ns)\n",
        "        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n",
        "\n",
        "        Ns = 2\n",
        "        Na = 3\n",
        "        \n",
        "        # Note we add a recharge option in state A with a negative reward (to have a well defined matrix-transition)\n",
        "        P = np.array([[[1, 0], [3/4, 1/4], [1, 0]], [[0,1],[1,0], [1,0]]])\n",
        "        self._R = np.array([[0,1,-0.5], [0, -1, 0]])\n",
        "\n",
        "        self.state_decoder  = {0: \"A\", 1: \"B\"}\n",
        "        self.action_decoder = {0: \"WAIT\", 1: \"SEARCH\", 2: \"RECHARGE\"}\n",
        "        \n",
        "        # Initialize base class\n",
        "        states = np.arange(Ns).tolist()\n",
        "        action_sets = [np.arange(Na).tolist()]*Ns\n",
        "        super().__init__(states, action_sets, P, gamma)\n",
        "\n",
        "    def reward_func(self, state, action, *_):\n",
        "        return self._R[state, action]\n",
        "\n",
        "    def reset(self, s=0):\n",
        "        self.state = s\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.sample_transition(self.state, action)\n",
        "        reward = self.reward_func(self.state, action, next_state)\n",
        "        done = False\n",
        "        info = {\"str\" : \"In {} do {} arrive at {} get {}\".format(\n",
        "            self.state_decoder[state],\n",
        "            self.action_decoder[action],\n",
        "            self.state_decoder[next_state],\n",
        "            reward )}\n",
        "        self.state = next_state\n",
        "\n",
        "        observation = next_state\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        prob = self.P[s,a,:]\n",
        "        s_ = self.RS.choice(self.states, p = prob)\n",
        "        return s_\n",
        "\n",
        "    def render_policy(self, policy):\n",
        "      if len(np.array(policy).shape) > 1:\n",
        "        policy = densify_policy(policy)\n",
        "\n",
        "      txt = \"\"\n",
        "      for i, a in enumerate(policy):\n",
        "        txt += \"In state {} perform {}\\n\".format(self.state_decoder[i], self.action_decoder[a])\n",
        "      return txt[:-1]\n",
        "\n",
        "    @property\n",
        "    def R(self):\n",
        "        return self._R\n",
        "  \n",
        "env = RobotEnv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdzoq4dLOX5C",
        "colab_type": "text"
      },
      "source": [
        "Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVHOHJJeOX5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.P.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.R.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=0, a=1): \", env.reward_func(0,1))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "print(env.render_policy(policy))\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward, \"   --> \" + info[\"str\"] if \"str\" in info else \"\") \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSWlX50bbrZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "statess=\n",
        "actions=\n",
        "ppi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YM7lLbRA68A",
        "colab_type": "text"
      },
      "source": [
        "## Useful functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sekFIFmm6V71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This functions ains at alternating between sparse and dense policy:\n",
        "# sparse policy allows to perform matrix completion pi \\in R^{SxA}. It is denoted pi in the code Ex: [[1, 0, 0], [0, 1, 0]])\n",
        "# dense policy is a determinist policy whose value are the indexed actions. It is the argmax of pi. It is denoted dpi in the code. Ex [0, 1] \n",
        "\n",
        "def sparsify_policy(policy, Na):\n",
        "  ### Turn a dense policy into a sparse one.\n",
        "  #  Ex: [0, 1], Na=3 -> [[1, 0, 0], [0, 1, 0]]\n",
        "  ###\n",
        "\n",
        "  Ns = len(policy)\n",
        "  sparse_policy = np.zeros(shape=(Ns, Na))\n",
        "  for i, a in enumerate(policy):\n",
        "    sparse_policy[i,a]=1\n",
        "  return sparse_policy\n",
        "\n",
        "def densify_policy(policy):\n",
        "  ### Turn a sparse determinist policy into a dense one.\n",
        "  #  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n",
        "  ###\n",
        "  return np.array(policy).argmax(axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dyTP0kkAXKo",
        "colab_type": "text"
      },
      "source": [
        "## Exercice : Policy Evaluation\n",
        "1. Evaluate the policy by solving the linear system\n",
        "2. Evaluate the policy through recursion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T0QX3tA2RRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Policy evaluation (exact)\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.P\n",
        "R = env.R\n",
        "gamma = env.gamma\n",
        "\n",
        "\n",
        "# Policy to evaluate\n",
        "# State A: Search\n",
        "# State B: Wait\n",
        "dense_policy = np.array([1, 0])\n",
        "pi = sparsify_policy(dense_policy, Na=env.Na)\n",
        "print(\"## old pi:\")\n",
        "print(pi)\n",
        "print(env.render_policy(pi))\n",
        "\n",
        "\n",
        "# Compute the dynamics given the policy\n",
        "Ppi =np.sum(P*np.expand_dims(pi,axis=-1), axis=1)\n",
        "#for a in range(len(state)):\n",
        "\n",
        "Rpi = np.sum(R*pi, axis=1)\n",
        "\n",
        "\n",
        "# Evaluate the policy\n",
        "Vpi = np.linalg.inv(np.identity(2)- gamma*Ppi).dot(Rpi)\n",
        "\n",
        "print(\"## Vpi: \", Vpi)\n",
        "\n",
        "\n",
        "# Compute the Q values\n",
        "Qpi = R + gamma * P.dot(Vpi)\n",
        "\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)\n",
        "\n",
        "\n",
        "# What is the next policy if we perform one step of policy improvment ?\n",
        "pi_new = Qpi.argmax(axis =1)\n",
        "\n",
        "print(\"## new pi:\")\n",
        "print(sparsify_policy(pi_new, Na=env.Na))\n",
        "print(env.render_policy(pi_new))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZb0Piis2YX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Policy evaluation (recursive)\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.P\n",
        "R = env.R\n",
        "gamma = env.gamma\n",
        "\n",
        "\n",
        "# Policy to evaluate\n",
        "dpi = np.array([1, 0])\n",
        "pi = sparsify_policy(dpi, Na=env.Na)\n",
        "\n",
        "\n",
        "# Stopping criterion -> maximum number of steps\n",
        "epsilon = 1e-3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Stopping criterion -> compute the infinite norm\n",
        "epsilon = 1e-3\n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0nl8OfLwpdC",
        "colab_type": "text"
      },
      "source": [
        "## New Environment!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDjMyANRs8Z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# New Maze environment\n",
        "# s: start\n",
        "# g: goal\n",
        "# x: negative reward state\n",
        "\n",
        "grid1 = [\n",
        "    ['', '', '', 'g'],\n",
        "    ['', 'x', '', ''],\n",
        "    ['s', '', '', '']\n",
        "]\n",
        "grid1_MAP = [\n",
        "    \"+-------+\",\n",
        "    \"| : : :G|\",\n",
        "    \"| :x: : |\",\n",
        "    \"|S: : : |\",\n",
        "    \"+-------+\",\n",
        "]\n",
        "\n",
        "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRUnx8EstjFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.P.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.R.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=0, a=1,s'=1): \", env.reward_func(0,1,1))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "dpi = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", dpi)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = dpi[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward) \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsXJD1qF74Iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dpi = np.random.randint(env.Na, size = (env.Ns,))\n",
        "env.render_policy(dpi)\n",
        "\n",
        "state = env.reset()\n",
        "env.render()\n",
        "for i in range(5):\n",
        "    action = dpi[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ugaKsHsOX5G",
        "colab_type": "text"
      },
      "source": [
        "## Exercice: Value Iteration\n",
        "1. Write a function applying the optimal Bellman operator on a provided Q function: $Q_1 = LQ_0, \\; Q_0\\in \\mathbb{R}^{S\\times A}$\n",
        "2. Write a function implementing Value Iteration (VI) with $\\infty$-norm stopping condition \n",
        "3. Evaluate the convergence of your estimate, i.e., plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n",
        "4. Evaluate the convergence of your estimate, i.e., plot the value $\\|\\pi_n - \\pi^\\star\\|_{\\infty}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvlitQzHD7Fc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# useful function\n",
        "def plot_infnorm(lst, star, name=\"V\"):\n",
        "  \n",
        "  lst = np.array(lst)\n",
        "  star = np.array(star)\n",
        "\n",
        "  # Compute inf norm\n",
        "  diff = np.absolute(lst - star).max(axis=1)\n",
        "  plt.figure()\n",
        "  plt.plot(diff)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Error')\n",
        "  plt.title(\"||{} - {}*||_inf\".format(name, name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKkDqSyBOX5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute Value Iteration\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.P\n",
        "R = env.R\n",
        "gamma = env.gamma\n",
        "epsilon = 5e-2\n",
        "\n",
        "# Prepare v, and storage\n",
        "v = np.zeros(env.Ns)\n",
        "v_all = []\n",
        "pi_all = []\n",
        "\n",
        "# iterate over the value\n",
        "while True:\n",
        "  \n",
        "  # Compute v_k\n",
        "  v = ...\n",
        "  v_all.append(v)\n",
        "\n",
        "  # Estimate Intermediate policy\n",
        "  dpi = ...\n",
        "  pi_all.append(dpi)\n",
        "\n",
        "  # stopping criterion \n",
        "  if ... : \n",
        "    break\n",
        "\n",
        "# compute optimal policy\n",
        "dpi = ...\n",
        "v_star = ...\n",
        "\n",
        "env.render_policy(dpi)\n",
        "\n",
        "plot_infnorm(v_all, v_star, name=\"V\")\n",
        "plot_infnorm(pi_all, dpi, name=\"Pi\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDT4iyJnAPIE",
        "colab_type": "text"
      },
      "source": [
        "## Exercice: Policy Iteration (Homework)\n",
        "1. Implement Policy iteration!\n",
        "2. Evaluate the convergence of your estimate, i.e., plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n",
        "3. Evaluate the convergence of your estimate, i.e., plot the value $\\|\\pi_n - \\pi^\\star\\|_{\\infty}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpBCGnqNE7l1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Retrieve the environment MDP\n",
        "P = env.P\n",
        "R = env.R\n",
        "gamma = env.gamma\n",
        "epsilon = 5e-2\n",
        "\n",
        "#Initialize policy\n",
        "dpi = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "print(env.render_policy(dpi))\n",
        "\n",
        "..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHrlauOwOX5L",
        "colab_type": "text"
      },
      "source": [
        "## Exercice: Q learning\n",
        "Q learning is a model-free algorithm for estimating the optimal Q-function online.\n",
        "It is an off-policy algorithm since the samples are collected with a policy that is (potentially) not the one associated to the estimated Q-function.\n",
        "\n",
        "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
        "  - Plot the error in Q-functions over iterations\n",
        "  - Plot the cumulative sum of rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCxJruBNOX5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------\n",
        "# Q-Learning\n",
        "# ---------------------------\n",
        "class QLearning:\n",
        "    \"\"\"\n",
        "    Q learning with epsilon-greedy exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate, epsilon, min_epsilon):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.Q = np.zeros((env.Ns, env.Na))\n",
        "        self.Nsa = np.zeros((env.Ns, env.Na))\n",
        "    \n",
        "    def sample_action(self, state, greedy=False):\n",
        "      pass\n",
        "    \n",
        "    def update(self, state, action, next_state, reward, done):\n",
        "      pass\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsmojgubLVL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q_learning = QLearning(env, gamma=env.gamma, learning_rate=1, epsilon=0.6, min_epsilon=0.1)\n",
        "\n",
        "# Define storage and variable\n",
        "q_all = []\n",
        "r_all = []\n",
        "pi_all = []\n",
        "max_steps = int(5e4)\n",
        "\n",
        "\n",
        "# main algorithmic loop\n",
        "state = env.reset()\n",
        "for t in range(max_steps):\n",
        "    \n",
        "  # Sample the action\n",
        "  ...\n",
        "  \n",
        "  # Sample the environment\n",
        "  ...\n",
        "  \n",
        "  # Update q-function\n",
        "  ...\n",
        "\n",
        "  # Store information \n",
        "  r_all.append(reward)\n",
        "  q_all.append(q_learning.Q)\n",
        "  pi_all.append(q_learning.Q.argmax(axis=1))\n",
        "  \n",
        "  state = next_state\n",
        "  if done:\n",
        "    state = env.reset()\n",
        "\n",
        "dpi = q_learning.Q.argmax(axis=1)\n",
        "print(env.render_policy(dpi))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysm4Cq_pxj5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "env.render()\n",
        "for i in range(5):\n",
        "    action = q_learning.sample_action(state, greedy=True)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QyNzQpLIpAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(q_learning.Q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi2j8vI4It8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_infnorm(pi_all, pi_all[-1], name=\"Pi\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xViQ6kpzJfw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pi = sparsify_policy(pi_all[-1], Na=env.Na)\n",
        "Ppi = np.sum(env.P * np.expand_dims(pi, axis=-1), axis=1)\n",
        "Rpi = np.sum(env.R * pi, axis=1)\n",
        "\n",
        "v_star = np.linalg.inv( np.identity(env.Ns) - gamma*Ppi).dot(Rpi)\n",
        "\n",
        "v_all = []\n",
        "for dpi_prev, q_prev in zip(pi_all, q_all):\n",
        "  pi_prev = sparsify_policy(dpi_prev, Na=env.Na)\n",
        "  v_prev = np.sum(pi_prev*q_prev, axis=1)\n",
        "  v_all.append(v_prev)\n",
        "\n",
        "plot_infnorm(v_all, v_star, name=\"V\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59xsIjpIOX5P",
        "colab_type": "text"
      },
      "source": [
        "## Exercice: SARSA (Homework)\n",
        "SARSA is another control algorithm. While Qlearning is off-policy, SARSA is on-policy.\n",
        "\n",
        "1. Implement SARSA with softmax (Gibbs) exploration and test the convergence to $Q^\\star$\n",
        "2. Plot the value $\\|V_n - V^\\star\\|_{\\infty}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gySnHtrsOX5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------\n",
        "# SARSA\n",
        "# ---------------------------\n",
        "class SARSA:\n",
        "    \"\"\"\n",
        "    SARSA with deacreasing epsilon for exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate, epsilon):\n",
        "      # Start with a random policy\n",
        "      pass\n",
        "    \n",
        "    def sample_action(self, state, greedy=False):\n",
        "      pass\n",
        "        \n",
        "    def update(self, state, action, next_state, next_action, reward):\n",
        "      pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be0d4nOBOX5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sarsa = SARSA(env, gamma=env.gamma, learning_rate=1., epsilon=1.)\n",
        "\n",
        "...\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}