{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of AIMS (Reinforce) Exercice.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "IMTSmNrMyFgH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berthine/Reinforcement-Learnin/blob/master/AIMS_(Reinforce)_Exercice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CWjie7h0zGdD"
      },
      "source": [
        "## Install, import and utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD74OdCFhUN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrO05StZyFee",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJvy13_UhLUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIQC-DpYhBpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "\n",
        "import random, os.path, math, glob, csv, base64, itertools, sys\n",
        "from pprint import pprint\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import io\n",
        "from IPython.display import HTML\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qkmxo4hhwIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following code is will be used to visualize the environments.\n",
        "\n",
        "def show_video(directory):\n",
        "    html = []\n",
        "    for mp4 in Path(directory).glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "    \n",
        "def make_seed(seed):\n",
        "    np.random.seed(seed=seed)\n",
        "    torch.manual_seed(seed=seed)\n",
        "  \n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NkcsoISWyFfu"
      },
      "source": [
        "## Reminder of the RL setting\n",
        "\n",
        "As always we will consider a MDP $M = (\\mathcal{S}, \\mathcal{A}, p, r, \\gamma)$ with:\n",
        "* $\\mathcal{S}$ the state space,\n",
        "* $\\mathcal{A}$ the action space,\n",
        "* $p(x^\\prime \\mid x, a)$ the transition probability,\n",
        "* $r(x, a, x^\\prime)$ the reward of the transition $(x, a, x^\\prime)$,\n",
        "* $\\gamma \\in [0,1)$ is the discount factor.\n",
        "\n",
        "A policy $\\pi$ is a mapping from the state space $\\mathcal{S}$ to the probability of selecting each action.\n",
        "\n",
        "The action value function of a policy is the overall expected reward from a state action. $Q^\\pi(s, a) = \\mathbb{E}_{\\tau \\sim \\pi}\\big[R(\\tau) \\mid s_0=s, a_0=a\\big]$ where $\\tau$ is an episode $(s_0, a_0, r_0, s_1, a_1, r_1, s_2, ..., s_T, a_T, r_T)$ with the actions drawn from $\\pi(s)$; $R(\\tau)$ is the random variable defined as the cumulative sum of the discounted reward.\n",
        "\n",
        "The goal is to maximize the agent's reward.\n",
        "\n",
        "$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\big[R(\\tau) \\big]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5OJnuROCyFfv"
      },
      "source": [
        "## Gym Environment\n",
        "\n",
        "In this lab and also the next one we are going to use the [OpenAI's Gym library](https://gym.openai.com/envs/). This library provides a large number of environments to test RL algorithm.\n",
        "\n",
        "We will focus on the **CartPole-v1** environment in this lab but we encourage you to also test your code on:\n",
        "* **Acrobot-v1**\n",
        "* **MountainCar-v0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5W7VaPHSyFfw"
      },
      "source": [
        "| Env Info          \t| CartPole-v1 \t| Acrobot-v1                \t| MountainCar-v0 \t|\n",
        "|-------------------\t|-------------\t|---------------------------\t|----------------\t|\n",
        "| **Observation Space** \t| Box(4)      \t| Box(6)                    \t| Box(2)         \t|\n",
        "| **Action Space**      \t| Discrete(2) \t| Discrete(3)               \t| Discrete(3)    \t|\n",
        "| **Rewards**           \t| 1 per step  \t| -1 if not terminal else 0 \t| -1 per step    \t|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hthuc34vyFfy"
      },
      "source": [
        "A gym environment is loaded with the command `env = gym.make(env_id)`. Once the environment is created, you need to reset it with `observation = env.reset()` and then you can interact with it using the method step: `observation, reward, done, info = env.step(action)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WTEGZ1nayFfz"
      },
      "source": [
        "### Carpole"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mnrKdIhsyFf0",
        "colab": {}
      },
      "source": [
        "# We load CartPole-v1\n",
        "env = gym.make('CartPole-v1')\n",
        "# We wrap it in order to save our experiment on a file.\n",
        "env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Eq_p3yKyFf3",
        "colab": {}
      },
      "source": [
        "done = False\n",
        "obs = env.reset()\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video(\"./gym-results\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L4wSjOadyFf8"
      },
      "source": [
        "### Acrobot-v1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FKGFYo3zyFf9",
        "colab": {}
      },
      "source": [
        "# We load Acrobot-v1\n",
        "env = gym.make('Acrobot-v1')\n",
        "# We wrap it in order to save our experiment on a file.\n",
        "env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pue8QgfFyFf_",
        "colab": {}
      },
      "source": [
        "done = False\n",
        "obs = env.reset()\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video(\"./gym-results\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZwRK_7ZtyFgC"
      },
      "source": [
        "### MountainCar-v0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZ9qetK-yFgC",
        "colab": {}
      },
      "source": [
        "# We load Acrobot-v1\n",
        "env = gym.make('MountainCar-v0')\n",
        "# We wrap it in order to save our experiment on a file.\n",
        "env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SbGbUebiyFgE",
        "colab": {}
      },
      "source": [
        "done = False\n",
        "obs = env.reset()\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video(\"./gym-results\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9B6HnSCOyFgH"
      },
      "source": [
        "## REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IMTSmNrMyFgH"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Reinforce is an actor-based **on policy** method. The policy $\\pi_{\\theta}$ is parametrized by a function approximator (e.g. a neural network).\n",
        "\n",
        "Recall: $$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\big[ \\sum_{t} \\gamma^t R_t \\mid x_0, \\pi \\big].$$\n",
        "\n",
        "To update the parameters $\\theta$ of the policy, one has to do gradient ascent: $\\theta_{k+1} = \\theta_{k} + \\alpha \\nabla_{\\theta}J(\\pi_{\\theta})|_{\\theta_{k}}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P2M2e8h-yFgI"
      },
      "source": [
        "### Policy Gradient Theorem\n",
        "\n",
        "$$ \\nabla_{\\theta} J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}\\left[{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) R(\\tau)}\\right]$$\n",
        "\n",
        "\n",
        "The policy gradient can be approximated with:\n",
        "$$ \\hat{g} = \\frac{1}{|\\mathcal{D}|} \\sum_{\\tau \\in \\mathcal{D}} \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) R(\\tau) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aGxnJVUkyFgI"
      },
      "source": [
        "### Implementation of the REINFORCE algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aNH3udIuyFgK",
        "colab": {}
      },
      "source": [
        "# This is your neural network model\n",
        "# You do not need to update it!\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, dim_observation, n_actions):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        self.n_actions = n_actions\n",
        "        self.dim_observation = dim_observation\n",
        "        \n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features=self.dim_observation, out_features=16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=16, out_features=8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=8, out_features=self.n_actions),\n",
        "            nn.Softmax(dim=0)\n",
        "        )\n",
        "        \n",
        "    def policy(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        return self.net(state)\n",
        "    \n",
        "    def sample_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        action = torch.multinomial(self.policy(state), 1)\n",
        "        return action.item()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aKGCDD4KyFgN"
      },
      "source": [
        "It is always nice to visualize the differents layers of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UzCLFgLEyFgN",
        "colab": {}
      },
      "source": [
        "# You can select your environment here\n",
        "env_id = 'MountainCar-v0'  #@param [\"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\"]\n",
        "env = gym.make(env_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMSSLFR4dO-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define you network\n",
        "model = Model(env.observation_space.shape[0], env.action_space.n)\n",
        "print(model)\n",
        "\n",
        "# Define your optimizer\n",
        "optimizer = torch.optim.Adam(model.net.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "num_steps = 50   # How many gradient step do we perform   \n",
        "batch_size = 64  # How many trajectories you have perfrom to estimate your gradient\n",
        "Tmax = 200       # Maximum length of your trajectory\n",
        "gamma = 1\n",
        "\n",
        "\n",
        "for step in range(num_steps):\n",
        "\n",
        "  # Initialize batch storage\n",
        "  batch_losses = torch.zeros(batch_size)\n",
        "  batch_returns = np.zeros(batch_size)\n",
        "\n",
        "  # Generate batch\n",
        "  for i in range(batch_size):\n",
        "\n",
        "    # Intialize environment\n",
        "    state = env.reset()\n",
        "\n",
        "    # Collect trajectory\n",
        "    for t in range(Tmax):   \n",
        "      ...\n",
        "\n",
        "    # Compute the trajectory of discounted rewards\n",
        "    # Ex: [0, 1, 1, 3] -> [5, 5, 4, 3] with gamma=1\n",
        "    ...\n",
        "\n",
        "    # Update the discounted return with baseline\n",
        "    ...\n",
        "\n",
        "    # Compute loss over one trajectory\n",
        "    policy_loss = torch.zeros(1)\n",
        "    ...\n",
        "\n",
        "    # Store batch data\n",
        "    batch_losses[i] = policy_loss\n",
        "    batch_returns[i] = ...\n",
        "\n",
        "  loss = batch_losses.mean()\n",
        "\n",
        "  # Update the agent\n",
        "  optimizer.zero_grad()  \n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print('Step {}/{} \\t reward: {:.2f} +/- {}'.format(\n",
        "        step, num_steps, np.mean(batch_returns), np.std(batch_returns)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XT5J1yyE6OG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This block displays your policy in a video\n",
        "video_env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)\n",
        "\n",
        "done = False\n",
        "reward_episode = 0\n",
        "obs = video_env.reset()\n",
        "while not done:\n",
        "    action = model.sample_action(state)\n",
        "    next_state, reward, done, info = video_env.step(action)\n",
        "    reward_episode += reward\n",
        "    state = next_state\n",
        "\n",
        "video_env.close()\n",
        "show_video(\"./gym-results\")\n",
        "\n",
        "print(f'Reward: {reward_episode}')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}