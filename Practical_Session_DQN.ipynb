{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Copy of Practical Session DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berthine/Reinforcement-Learnin/blob/master/Practical_Session_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opZzyYN7fwxD",
        "colab_type": "text"
      },
      "source": [
        "## Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbV50a2jfwxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA_0eutEfwxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqhcwbE3fwxX",
        "colab_type": "code",
        "outputId": "084d112b-c53f-4bbf-b0d8-282f2d94dd3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (45.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFQNOxGsfwxc",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q-Learning (DQN)\n",
        "\n",
        "\n",
        "In DQN, the $Q$-function is parameterized by a neural network of parameters $\\theta$. The network takes as input a state $s$ and outputs $Q(s, a, \\theta)$ for all actions $a$. \n",
        "\n",
        "The network is trained in way that is similar to Fitted Q Iteration. At each time $T$, the agent has observed the transitions $(s_t, a_t, r_t, s_t')_{t=1}^T$, which are stored in a __replay buffer__.\n",
        "\n",
        "In addition to the network with parameters $\\theta$, DQN keeps another network with the same architecture and parameters $\\tilde{\\theta}$, called __target network__. \n",
        "To update the parameters $\\theta$, we sample $N$ transitions from the __replay buffer__, we define the loss \n",
        "\n",
        "$$\n",
        "L(\\theta) = \\sum_{i=1}^N [Q(s_i, a_i, \\theta) - (r_i + \\gamma\\max_{a'}Q(s'_i,a', \\tilde{\\theta}))]^2\n",
        "$$\n",
        "\n",
        "and update \n",
        "\n",
        "$$\n",
        "\\theta \\gets \\theta + \\eta \\nabla L(\\theta).\n",
        "$$\n",
        "\n",
        "\n",
        "Every $C$ iterations, the target network is updated as $\\tilde{\\theta} \\gets \\theta$. \n",
        "\n",
        "At each time $t$, DQN updates the networks as described above, selects an action according to an $\\epsilon$-greedy policy, plays the action and stores the new data in the replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrBtkvHqfwxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "\n",
        "import random, os.path, math, glob, csv, base64, itertools, sys\n",
        "from pprint import pprint\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import io\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz3beIAHfwxi",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Define the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV7lKaVGfwxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Environment\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# Discount factor\n",
        "GAMMA = 0.99\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 256\n",
        "# Capacity of the replay buffer\n",
        "BUFFER_CAPACITY = 10000\n",
        "# Update target net every ... episodes\n",
        "UPDATE_TARGET_EVERY = 20\n",
        "\n",
        "# Initial value of epsilon\n",
        "EPSILON_START = 1.0\n",
        "# Parameter to decrease epsilon\n",
        "DECREASE_EPSILON = 200\n",
        "# Minimum value of epislon\n",
        "EPSILON_MIN = 0.05\n",
        "\n",
        "# Number of training episodes\n",
        "N_EPISODES = 200\n",
        "\n",
        "# Learning rate\n",
        "LEARNING_RATE = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4fVXZSafwxo",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Define the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH1tb1wIfwxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, reward, next_state)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFgVBS8Gfwxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create instance of replay buffer\n",
        "replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6ki1HpXfwxw",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Define the neural network architecture, objective and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP2R3fZ8fwxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic neural net.\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_Oz9uvzfwx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create network and target network\n",
        "hidden_size = 64\n",
        "obs_size = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "q_net = Net(obs_size, hidden_size, n_actions)\n",
        "target_net = Net(obs_size, hidden_size, n_actions)\n",
        "\n",
        "# objective and optimizer\n",
        "objective = nn.MSELoss()\n",
        "optimizer = optim.Adam(params=q_net.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYeNGkE0fwx5",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Implement DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQqeJNoOfwx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "#  Some useful functions\n",
        "#\n",
        "\n",
        "def get_q(states):\n",
        "    \"\"\"\n",
        "    Compute Q function for a list of states\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        states_v = torch.FloatTensor([states])\n",
        "        output = q_net.forward(states_v).data.numpy()  # shape (1, len(states), n_actions)\n",
        "    return output[0, :, :]  # shape (len(states), n_actions)\n",
        "\n",
        "def eval_dqn(n_sim=5):\n",
        "    \"\"\"\n",
        "    Monte Carlo evaluation of DQN agent\n",
        "    \"\"\"\n",
        "    rewards = np.zeros(n_sim)\n",
        "    copy_env = deepcopy(env) # Important!\n",
        "    # Loop over number of simulations\n",
        "    for sim in range(n_sim):\n",
        "      state = copy_env.reset()\n",
        "      done = False\n",
        "      while not done:\n",
        "        action = choose_action(state, 0)\n",
        "        next_state, reward, done, _ = copy_env.step(action)\n",
        "        # update sum of rewards\n",
        "        rewards[sim] += reward\n",
        "        state = next_state\n",
        "    return rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvgkbH0Vfwx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def choose_action(state, epsilon):\n",
        "    \"\"\"\n",
        "    TO BE IMPLEMENTED\n",
        "    \n",
        "    Return action according to an epsilon-greedy exploration policy\n",
        "    \"\"\"\n",
        "    q_state = get_q([state])[0] # array of shape (n_actions,)\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "      action = env.action_space.sample() # random action\n",
        "    else:\n",
        "      action = q_state.argmax()\n",
        "    return action\n",
        "    \n",
        "\n",
        "def update(state, action, reward, next_state, done):\n",
        "    \"\"\"\n",
        "    TO BE COMPLETED\n",
        "    \"\"\"\n",
        "    \n",
        "    # add data to replay buffer\n",
        "    if done:\n",
        "        next_state = None\n",
        "    replay_buffer.push(state, action, reward, next_state)\n",
        "    \n",
        "    if len(replay_buffer) < BATCH_SIZE:\n",
        "        return np.inf\n",
        "    \n",
        "    # get batch\n",
        "    # transitions = list of (state, action, reward, next_state)\n",
        "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
        "\n",
        "    # 1st thing: compute Q(s_i, a_i, theta) for all (s_i, a_i)\n",
        "    # in the batch\n",
        "    \n",
        "    # Build tensor with s_i and tensor with a_i\n",
        "    batch_states = torch.FloatTensor( \n",
        "                    [ transitions[ii][0] for ii in range(BATCH_SIZE) ]\n",
        "                    )\n",
        "    batch_actions = torch.LongTensor(  # type is important (Long) \n",
        "                    [ transitions[ii][1] for ii in range(BATCH_SIZE) ]\n",
        "                    )\n",
        "    batch_rewards = torch.FloatTensor( \n",
        "                    [ transitions[ii][2] for ii in range(BATCH_SIZE) ]\n",
        "                    )\n",
        "\n",
        "    non_final_mask = torch.tensor([(transitions[ii][3] is not None) \n",
        "                                   for ii in range(BATCH_SIZE)], dtype=torch.bool)\n",
        "    non_final_next_states = torch.FloatTensor(\n",
        "            [transitions[ii][3]  for ii in range(BATCH_SIZE) \n",
        "            if transitions[ii][3] is not None])\n",
        "        \n",
        "    next_state_values = torch.zeros(BATCH_SIZE)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    state_action_values = q_net(batch_states).gather(1, batch_actions.view(-1, 1))\n",
        "\n",
        "    # # A simpler (but slower) way to compute all these things\n",
        "    # for ii in range(BATCH_SIZE):\n",
        "    #   state_ii = transitions[ii][0]\n",
        "    #   action_ii = transitions[ii][1]\n",
        "    #   reward_ii = transitions[ii][2]\n",
        "    #   next_state_ii = transitions[ii][3]\n",
        "      \n",
        "    #   next_value = 0\n",
        "    #   if next_state_ii is not None:\n",
        "    #     next_value = # max_a Q(next_state, a) with target net\n",
        "      \n",
        "\n",
        "    #   values[ii] = # Q(state_ii)[action_ii]  with q_net\n",
        "    #   targets[ii] = reward_ii + GAMMA*next_value\n",
        "    \n",
        "\n",
        "    # Compute loss - TO BE IMPLEMENTED!\n",
        "    values  = state_action_values\n",
        "    targets = batch_rewards + GAMMA*next_state_values\n",
        "    loss = objective(values, targets.unsqueeze(1))\n",
        "     \n",
        "    # Optimize the model - UNCOMMENT!\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.data.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUzuCwdcfwyC",
        "colab_type": "code",
        "outputId": "2b428f92-7ff6-410f-d97b-680a89e0b8b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        }
      },
      "source": [
        "#\n",
        "# Train\n",
        "# \n",
        "\n",
        "EVAL_EVERY = 5\n",
        "REWARD_THRESHOLD = 199\n",
        "\n",
        "def train():\n",
        "    state = env.reset()\n",
        "    epsilon = EPSILON_START\n",
        "    ep = 0\n",
        "    total_time = 0\n",
        "    while ep < N_EPISODES:\n",
        "        action = choose_action(state, epsilon) # eps-greedy \n",
        "\n",
        "        # take action and update replay buffer and networks\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        loss = update(state, action, reward, next_state, done)\n",
        "\n",
        "        # update state\n",
        "        state = next_state\n",
        "\n",
        "        # end episode if done\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            ep   += 1\n",
        "            if ( (ep+1)% EVAL_EVERY == 0):\n",
        "                rewards = eval_dqn()\n",
        "                print(\"episode =\", ep+1, \", reward = \", np.mean(rewards),\n",
        "                      \"loss = \", loss)\n",
        "                if np.mean(rewards) >= REWARD_THRESHOLD:\n",
        "                    break\n",
        "\n",
        "            # update target network\n",
        "            if ep % UPDATE_TARGET_EVERY == 0:\n",
        "                target_net.load_state_dict(q_net.state_dict())\n",
        "            # decrease epsilon\n",
        "            epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN) * \\\n",
        "                            np.exp(-1. * ep / DECREASE_EPSILON )    \n",
        "\n",
        "        total_time += 1\n",
        "\n",
        "train()\n",
        "rewards = eval_dqn(20)\n",
        "print(\"\")\n",
        "print(\"mean reward after training = \", np.mean(rewards))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode = 5 , reward =  22.8 loss =  inf\n",
            "episode = 10 , reward =  20.2 loss =  inf\n",
            "episode = 15 , reward =  22.8 loss =  inf\n",
            "episode = 20 , reward =  8.8 loss =  0.0015220157\n",
            "episode = 25 , reward =  9.2 loss =  0.019437987\n",
            "episode = 30 , reward =  10.8 loss =  0.01826595\n",
            "episode = 35 , reward =  10.4 loss =  0.009716518\n",
            "episode = 40 , reward =  10.4 loss =  0.013238778\n",
            "episode = 45 , reward =  10.2 loss =  0.03671535\n",
            "episode = 50 , reward =  13.2 loss =  0.022972265\n",
            "episode = 55 , reward =  11.6 loss =  0.019763853\n",
            "episode = 60 , reward =  10.0 loss =  0.027014093\n",
            "episode = 65 , reward =  53.8 loss =  0.053124044\n",
            "episode = 70 , reward =  40.4 loss =  0.03706641\n",
            "episode = 75 , reward =  38.6 loss =  0.029938579\n",
            "episode = 80 , reward =  22.6 loss =  0.02353244\n",
            "episode = 85 , reward =  23.2 loss =  0.08750124\n",
            "episode = 90 , reward =  23.4 loss =  0.092875585\n",
            "episode = 95 , reward =  26.8 loss =  0.048008576\n",
            "episode = 100 , reward =  14.4 loss =  0.057054985\n",
            "episode = 105 , reward =  12.0 loss =  0.099775255\n",
            "episode = 110 , reward =  35.4 loss =  0.14979899\n",
            "episode = 115 , reward =  38.4 loss =  0.07681377\n",
            "episode = 120 , reward =  71.2 loss =  0.060583115\n",
            "episode = 125 , reward =  90.4 loss =  0.09103316\n",
            "episode = 130 , reward =  34.2 loss =  0.18320529\n",
            "episode = 135 , reward =  102.4 loss =  0.1076012\n",
            "episode = 140 , reward =  81.6 loss =  0.10801439\n",
            "episode = 145 , reward =  94.4 loss =  0.09629414\n",
            "episode = 150 , reward =  97.2 loss =  0.12003446\n",
            "episode = 155 , reward =  75.8 loss =  0.14302367\n",
            "episode = 160 , reward =  69.4 loss =  0.20683587\n",
            "episode = 165 , reward =  118.6 loss =  0.14654864\n",
            "episode = 170 , reward =  111.6 loss =  0.22420771\n",
            "episode = 175 , reward =  101.6 loss =  0.12770595\n",
            "episode = 180 , reward =  101.2 loss =  0.45960236\n",
            "episode = 185 , reward =  106.2 loss =  0.2696698\n",
            "episode = 190 , reward =  110.6 loss =  0.25399926\n",
            "episode = 195 , reward =  102.8 loss =  0.31500918\n",
            "episode = 200 , reward =  107.2 loss =  0.18262997\n",
            "\n",
            "mean reward after training =  108.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhWv6ThkfwyG",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0sGUpzJfwyH",
        "colab_type": "code",
        "outputId": "50fca283-0da2-439d-e660-c7e4e832f57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "def show_video(directory):\n",
        "    html = []\n",
        "    for mp4 in Path(directory).glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "    \n",
        "def make_seed(seed):\n",
        "    np.random.seed(seed=seed)\n",
        "    torch.manual_seed(seed=seed)\n",
        "  \n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2Fhe2iefwyM",
        "colab_type": "code",
        "outputId": "2f5b8c5f-fe33-443f-e468-0fb08c64ac9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)\n",
        "for episode in range(1):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    while not done:\n",
        "        action = choose_action(state, 0) # MODIFY THIS PART TO COMPUTE THE ACTION WITH DQN\n",
        "        state, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video(\"./gym-results\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"gym-results/openaigym.video.0.891.video000000.mp4\" autoplay \n",
              "                      loop controls style=\"height: 400px;\">\n",
              "                      <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "                 </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuYUJDQ6VOOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}